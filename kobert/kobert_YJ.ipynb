{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert import download\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import os\n",
    "from transformers import BertModel\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pytorch_kobert_model(ctx=\"cpu\", cachedir=\".cache\"):\n",
    "    \n",
    "    def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n",
    "        bertmodel = BertModel.from_pretrained(model_path)\n",
    "        device = torch.device(ctx)\n",
    "        bertmodel.to(device)\n",
    "        bertmodel.eval()\n",
    "        vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(\n",
    "            vocab_file, padding_token=\"[PAD]\"\n",
    "        )\n",
    "        return bertmodel, vocab_b_obj\n",
    "\n",
    "    pytorch_kobert = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBERT/models/kobert_v1.zip\",\n",
    "        \"chksum\": \"411b242919\",  # 411b2429199bc04558576acdcac6d498\n",
    "    }\n",
    "    \n",
    "    # download model\n",
    "    model_info = pytorch_kobert\n",
    "    model_path, is_cached = download(\n",
    "        model_info[\"url\"], model_info[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "    cachedir_full = os.path.expanduser(cachedir)\n",
    "    zipf = ZipFile(os.path.expanduser(model_path))\n",
    "    zipf.extractall(path=cachedir_full)\n",
    "    model_path = os.path.join(os.path.expanduser(cachedir), \"kobert_from_pretrained\")\n",
    "    # download vocab\n",
    "    vocab_path = get_tokenizer()\n",
    "    return get_kobert_model(model_path, vocab_path, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert\\.cache\\kobert_v1.zip\n",
      "using cached model. C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert\\input_data.csv',index_col=0)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(df['상승하락'])\n",
    "df['상승하락'] = encoder.transform(df['상승하락'])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['제목'] = df['제목'].astype(str)\n",
    "df['상승하락'] = df['상승하락'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values.tolist()\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173617\n",
      "43405\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size = 0.2, shuffle = False, random_state=777)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 8 #64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5 #5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-2 # 5e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch용 DataLoader 사용\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size) # num_workers=5\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size) #num_workers=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 2, # softmax 사용 <- binary일 경우는 2\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 옵티마이저 선언\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0a9cbc23d846c69c30c7d6cd547e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21703), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 5.344594478607178 train acc 0.125\n",
      "epoch 1 batch id 201 loss 7.07663106918335 train acc 0.4141791044776119\n",
      "epoch 1 batch id 401 loss 4.286240577697754 train acc 0.5090399002493765\n",
      "epoch 1 batch id 601 loss 7.69052267074585 train acc 0.5324459234608985\n",
      "epoch 1 batch id 801 loss 10.152761459350586 train acc 0.533083645443196\n",
      "epoch 1 batch id 1001 loss 0.001449945499189198 train acc 0.5555694305694305\n",
      "epoch 1 batch id 1201 loss 0.0023461789824068546 train acc 0.5600541215653622\n",
      "epoch 1 batch id 1401 loss 3.475349187850952 train acc 0.547644539614561\n",
      "epoch 1 batch id 1601 loss 0.00077782291918993 train acc 0.5455965021861336\n",
      "epoch 1 batch id 1801 loss 0.08194095641374588 train acc 0.5468489727928928\n",
      "epoch 1 batch id 2001 loss 11.463945388793945 train acc 0.5503498250874562\n",
      "epoch 1 batch id 2201 loss 0.08013486117124557 train acc 0.5557133121308496\n",
      "epoch 1 batch id 2401 loss 0.0 train acc 0.5550812161599333\n",
      "epoch 1 batch id 2601 loss 0.0 train acc 0.5590157631680123\n",
      "epoch 1 batch id 2801 loss 24.446853637695312 train acc 0.5576133523741521\n",
      "epoch 1 batch id 3001 loss 0.15188366174697876 train acc 0.5604798400533155\n",
      "epoch 1 batch id 3201 loss 0.0 train acc 0.5588487972508591\n",
      "epoch 1 batch id 3401 loss 1.7910468159243464e-05 train acc 0.5678476918553367\n",
      "epoch 1 batch id 3601 loss 0.0 train acc 0.566439877811719\n",
      "epoch 1 batch id 3801 loss 0.0 train acc 0.5629439621152328\n",
      "epoch 1 batch id 4001 loss 0.0 train acc 0.5676705823544114\n",
      "epoch 1 batch id 4201 loss 0.00012513513502199203 train acc 0.5647167341109259\n",
      "epoch 1 batch id 4401 loss 27.786863327026367 train acc 0.5746989320608953\n",
      "epoch 1 batch id 4601 loss 28.22040367126465 train acc 0.5757172353836123\n",
      "epoch 1 batch id 4801 loss 0.0 train acc 0.578264944803166\n",
      "epoch 1 batch id 5001 loss 33.649444580078125 train acc 0.5734353129374126\n",
      "epoch 1 batch id 5201 loss 0.0 train acc 0.5655162468756009\n",
      "epoch 1 batch id 5401 loss 0.08733894675970078 train acc 0.5675337900388817\n",
      "epoch 1 batch id 5601 loss 17.140544891357422 train acc 0.5693626138189609\n",
      "epoch 1 batch id 5801 loss 16.577781677246094 train acc 0.5625323220134459\n",
      "epoch 1 batch id 6001 loss 53.84589385986328 train acc 0.5597817030494917\n",
      "epoch 1 batch id 6201 loss 124.05902862548828 train acc 0.562489920980487\n",
      "epoch 1 batch id 6401 loss 0.0 train acc 0.5665716294329011\n",
      "epoch 1 batch id 6601 loss 1.818425178527832 train acc 0.569591728525981\n",
      "epoch 1 batch id 6801 loss 0.0 train acc 0.5739964711071901\n",
      "epoch 1 batch id 7001 loss 184.18653869628906 train acc 0.5733466647621769\n",
      "epoch 1 batch id 7201 loss 0.0 train acc 0.571726149145952\n",
      "epoch 1 batch id 7401 loss 1.038794994354248 train acc 0.5695683015808675\n",
      "epoch 1 batch id 7601 loss 2.4710893630981445 train acc 0.5728358110774898\n",
      "epoch 1 batch id 7801 loss 0.0 train acc 0.57168952698372\n",
      "epoch 1 batch id 8001 loss 107.93974304199219 train acc 0.5750218722659668\n",
      "epoch 1 batch id 8201 loss 91.33163452148438 train acc 0.5764845750518229\n",
      "epoch 1 batch id 8401 loss 0.0008725407533347607 train acc 0.5765533865016069\n",
      "epoch 1 batch id 8601 loss 0.0 train acc 0.5783920474363446\n",
      "epoch 1 batch id 8801 loss 0.0 train acc 0.5770793091694125\n",
      "epoch 1 batch id 9001 loss 0.0 train acc 0.579880013331852\n",
      "epoch 1 batch id 9201 loss 0.006574550177901983 train acc 0.5792441039017499\n",
      "epoch 1 batch id 9401 loss 138.7842559814453 train acc 0.5800180831826401\n",
      "epoch 1 batch id 9601 loss 3.9133496284484863 train acc 0.5757733569419852\n",
      "epoch 1 batch id 9801 loss 0.0 train acc 0.5786909499030711\n",
      "epoch 1 batch id 10001 loss 0.0 train acc 0.5822292770722928\n",
      "epoch 1 batch id 10201 loss 0.0 train acc 0.5812787961964513\n",
      "epoch 1 batch id 10401 loss 4.91737353058852e-07 train acc 0.5811220074992789\n",
      "epoch 1 batch id 10601 loss 28.5474910736084 train acc 0.5829284973115744\n",
      "epoch 1 batch id 10801 loss 68.40176391601562 train acc 0.5834413480233311\n",
      "epoch 1 batch id 11001 loss 123.47088623046875 train acc 0.5804472320698119\n",
      "epoch 1 batch id 11201 loss 0.0 train acc 0.5810083921078475\n",
      "epoch 1 batch id 11401 loss 101.93721771240234 train acc 0.5820213139198316\n",
      "epoch 1 batch id 11601 loss 106.68982696533203 train acc 0.5820942160158608\n",
      "epoch 1 batch id 11801 loss 1.6817479133605957 train acc 0.5812537073129396\n",
      "epoch 1 batch id 12001 loss 94.92796325683594 train acc 0.5828368469294225\n",
      "epoch 1 batch id 12201 loss 5.017185688018799 train acc 0.5832308827145316\n",
      "epoch 1 batch id 12401 loss 0.23757359385490417 train acc 0.5834610112087735\n",
      "epoch 1 batch id 12601 loss 0.0 train acc 0.5849138957225617\n",
      "epoch 1 batch id 12801 loss 0.0 train acc 0.5887821263963753\n",
      "epoch 1 batch id 13001 loss 0.0 train acc 0.5894450426890239\n",
      "epoch 1 batch id 13201 loss 0.0 train acc 0.5903814104992046\n",
      "epoch 1 batch id 13401 loss 0.0 train acc 0.5906089097828521\n",
      "epoch 1 batch id 13601 loss 0.0 train acc 0.5933111535916477\n",
      "epoch 1 batch id 13801 loss 260.15350341796875 train acc 0.5937522643286718\n",
      "epoch 1 batch id 14001 loss 0.0 train acc 0.5945646739518606\n",
      "epoch 1 batch id 14201 loss 278.606201171875 train acc 0.5937081895641152\n",
      "epoch 1 batch id 14401 loss 0.007086728233844042 train acc 0.5929970140962433\n",
      "epoch 1 batch id 14601 loss 60.68727493286133 train acc 0.5909441134168892\n",
      "epoch 1 batch id 14801 loss 0.0 train acc 0.5898756840754004\n",
      "epoch 1 batch id 15001 loss 313.1561279296875 train acc 0.5908856076261583\n",
      "epoch 1 batch id 15201 loss 0.0 train acc 0.5908904019472403\n",
      "epoch 1 batch id 15401 loss 1.4319484233856201 train acc 0.5900185052918642\n",
      "epoch 1 batch id 15601 loss 0.0 train acc 0.5897298250112172\n",
      "epoch 1 batch id 15801 loss 0.0 train acc 0.5894959179798747\n",
      "epoch 1 batch id 16001 loss 0.0 train acc 0.5882601087432036\n",
      "epoch 1 batch id 16201 loss 0.0 train acc 0.5877955064502192\n",
      "epoch 1 batch id 16401 loss 0.0 train acc 0.5886683738796414\n",
      "epoch 1 batch id 16601 loss 0.0 train acc 0.5890910186133366\n",
      "epoch 1 batch id 16801 loss 1.2812321186065674 train acc 0.5880751145765133\n",
      "epoch 1 batch id 17001 loss 0.0 train acc 0.5882374566201988\n",
      "epoch 1 batch id 17201 loss 0.0 train acc 0.5878291959769781\n",
      "epoch 1 batch id 17401 loss 3.334552526473999 train acc 0.5891543589448882\n",
      "epoch 1 batch id 17601 loss 0.0 train acc 0.5903641838531901\n",
      "epoch 1 batch id 17801 loss 145.41989135742188 train acc 0.5887239480928038\n",
      "epoch 1 batch id 18001 loss 551.5966186523438 train acc 0.5894533637020165\n",
      "epoch 1 batch id 18201 loss 189.26806640625 train acc 0.590922202076809\n",
      "epoch 1 batch id 18401 loss 0.0 train acc 0.590572523232433\n",
      "epoch 1 batch id 18601 loss 0.0 train acc 0.5888258695769044\n",
      "epoch 1 batch id 18801 loss 0.0 train acc 0.58844609329291\n",
      "epoch 1 batch id 19001 loss 622.6770629882812 train acc 0.5885348139571601\n",
      "epoch 1 batch id 19201 loss 165.67721557617188 train acc 0.5878469871360866\n",
      "epoch 1 batch id 19401 loss 0.0 train acc 0.5872184423483325\n",
      "epoch 1 batch id 19601 loss 0.0 train acc 0.5867494005407887\n",
      "epoch 1 batch id 19801 loss 0.0 train acc 0.5859868188475329\n",
      "epoch 1 batch id 20001 loss 6.028116226196289 train acc 0.587014399280036\n",
      "epoch 1 batch id 20201 loss 2.6731855869293213 train acc 0.5882567694668581\n",
      "epoch 1 batch id 20401 loss 0.0 train acc 0.5882616048232929\n",
      "epoch 1 batch id 20601 loss 426.8255615234375 train acc 0.5887032182903743\n",
      "epoch 1 batch id 20801 loss 85.93097686767578 train acc 0.5875378587567905\n",
      "epoch 1 batch id 21001 loss 0.0 train acc 0.5871089471929908\n",
      "epoch 1 batch id 21201 loss 0.0 train acc 0.5871951794726664\n",
      "epoch 1 batch id 21401 loss 0.0 train acc 0.5890145320312135\n",
      "epoch 1 batch id 21601 loss 3.5078139305114746 train acc 0.5878431554094717\n",
      "\n",
      "epoch 1 train acc 0.5880869004285122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7d77c5abd04858a08e7a88afee7847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5426), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-2f98a2981eab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m#preds.to_csv(r'C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert' + '/' + e + '_preds' + '.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0me\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_labels'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch {} test acc {}, temp_df {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "# 모델 학습 시작\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval() # 평가 모드로 변경\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        _,preds = torch.max(out.data, 1)\n",
    "        \n",
    "        #temp_df.concat(preds)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "        \n",
    "    #preds.to_csv(r'C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert' + '/' + e + '_preds' + '.csv')    \n",
    "    #label.to_csv(r'C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert' + '/' + e + '_labels' + '.csv')\n",
    "    print(\"epoch {} test acc {}, temp_df {}\".format(e+1, test_acc / (batch_id+1), temp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
