{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "import gluonnlp as nlp\n",
    "from kobert import download, get_tokenizer\n",
    "import torch\n",
    "from kobert import get_pytorch_kobert_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pytorch_kobert_model(ctx=\"cpu\", cachedir=\".cache\"):\n",
    "    \n",
    "    def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n",
    "        bertmodel = BertModel.from_pretrained(model_path)\n",
    "        device = torch.device(ctx)\n",
    "        bertmodel.to(device)\n",
    "        bertmodel.eval()\n",
    "        vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(\n",
    "            vocab_file, padding_token=\"[PAD]\"\n",
    "        )\n",
    "        return bertmodel, vocab_b_obj\n",
    "\n",
    "    pytorch_kobert = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBERT/models/kobert_v1.zip\",\n",
    "        \"chksum\": \"411b242919\",  # 411b2429199bc04558576acdcac6d498\n",
    "    }\n",
    "\n",
    "    # download model\n",
    "    model_info = pytorch_kobert\n",
    "    model_path, is_cached = download(\n",
    "        model_info[\"url\"], model_info[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "    cachedir_full = os.path.expanduser(cachedir)\n",
    "    zipf = ZipFile(os.path.expanduser(model_path))\n",
    "    zipf.extractall(path=cachedir_full)\n",
    "    model_path = os.path.join(os.path.expanduser(cachedir), \"kobert_from_pretrained\")\n",
    "    # download vocab\n",
    "    vocab_path = get_tokenizer()\n",
    "    return get_kobert_model(model_path, vocab_path, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert\\.cache\\kobert_v1.zip\n",
      "using cached model. C:\\Users\\WIN10\\Documents\\GitHub\\KoBERT\\kobert\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "torch.Size([2, 768])\n",
      "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],\n",
      "        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],\n",
      "        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "model, vocab = get_pytorch_kobert_model()\n",
    "sequence_output, pooled_output = model(input_ids, input_mask, token_type_ids)\n",
    "print(pooled_output.shape)\n",
    "print(vocab)\n",
    "print(sequence_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
